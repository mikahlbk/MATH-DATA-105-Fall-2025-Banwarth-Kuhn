{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0371b3d-6859-42ed-bb1a-4dbd5deaf63f",
   "metadata": {},
   "source": [
    "Suppose a business has collected data showing the relationship between dollars spent on advertising (in thousands) versus corresponding sales revenue (in millions). Now, they want to model the relationship between their advertising spending and sales performance. They've hired you to develop this predictive model using the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cde76e-4679-4cec-8e69-2864fef3ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dollars_spent = [1,2,3,4]\n",
    "sales_revenue = [2,4,6,8]\n",
    "plt.scatter(x_vals,y_vals,marker= \"o\",color = 'blue',label = \"(x,y)- Observed Data\")\n",
    "plt.xlabel(\"Dollars spent on advertising (thousands)\")\n",
    "plt.ylabel(\"Sales revenue (millions)\")\n",
    "plt.legend()\n",
    "plt.ylim([0, 10])\n",
    "plt.xlim([0, 5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf1c98-968e-4136-ae64-b894752b44dd",
   "metadata": {},
   "source": [
    "We decided the relationship is linear. This means our predictive model takes the form\n",
    "$$\\hat{y} = mx.$$\n",
    "Note we are assuming the inital value is 0 for simplicity.\n",
    "\n",
    "For example, for $m = .5$ we get the model\n",
    "$$\\hat{y} = .5x$$ \n",
    "which is plotted along with the observed data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf3374-8593-4431-96a5-89cfa8bf1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dollars_spent = np.array([1, 2, 3, 4])\n",
    "sales_revenue = np.array([2, 4, 6, 8])\n",
    "\n",
    "model_func = lambda x: 0.5 * x\n",
    "predicted = model_func(dollars_spent)\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(dollars_spent, sales_revenue, color='blue', label='(x, y) - Observed Data')\n",
    "plt.scatter(dollars_spent, predicted, color='red', label=r'$(x, \\hat{y})$ - Predictions')\n",
    "\n",
    "# Dashed lines between observed and predicted points\n",
    "for x, y_obs, y_pred in zip(dollars_spent, sales_revenue, predicted):\n",
    "    plt.plot([x, x], [y_obs, y_pred], 'k--', linewidth=1)  # vertical dashed line\n",
    "\n",
    "plt.xlabel(\"Dollars spent on advertising (thousands)\")\n",
    "plt.ylabel(\"Sales revenue (millions)\")\n",
    "plt.legend()\n",
    "plt.ylim([0, 10])\n",
    "plt.xlim([0, 5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27f021-36e5-4feb-a0db-b547d7e4d514",
   "metadata": {},
   "source": [
    "The dashed lines represent the distance between the observed sales and the predicted sales for each input value. Using this distance as a guide, we developed the mean-squared-error (MSE) cost function\n",
    "$$C(m) = \\frac{1}{4}\\sum_{i=1}^4 (y_i-mx_i)^2$$\n",
    "that computes the total error for a given choice of $m$.\n",
    "\n",
    "In class we computed $C(m)$ for a range of $m$ values and produced the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7dfb8-2bcc-4599-a566-e76154ccfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Example data: advertising dollars vs. sales revenue\n",
    "# ---------------------------------------------\n",
    "dollars_spent = np.array([1, 2, 3, 4])\n",
    "sales_revenue = np.array([2, 4, 6, 8])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Create a range of possible slope values (our model parameters)\n",
    "# ---------------------------------------------\n",
    "# We'll test slope values from 0 to 5 in steps of 0.5\n",
    "slope_choices = np.arange(0, 5.5, 0.5)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Generate predictions for each slope\n",
    "# ---------------------------------------------\n",
    "# Each slope defines a line:  ŷ = m * x\n",
    "# We compute predicted y-values for each slope and each data point\n",
    "predictions = slope_choices[:, np.newaxis] * dollars_spent\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Compute the cost for each slope\n",
    "# ---------------------------------------------\n",
    "# We'll use Mean Squared Error (MSE):\n",
    "#     Cost = (1/n) * Σ (y - ŷ)²\n",
    "cost = (1 / len(sales_revenue)) * np.sum((sales_revenue - predictions) ** 2, axis=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Plot the cost function\n",
    "# ---------------------------------------------\n",
    "plt.plot(slope_choices, cost, marker=\"o\", label=\"Cost\")\n",
    "plt.xlabel(\"Slope value (m)\")\n",
    "plt.ylabel(\"Cost (Mean Squared Error)\")\n",
    "plt.xlim([0, 6])\n",
    "plt.ylim([0, 50])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a5feb-53fb-4bee-b618-a65f32a1740c",
   "metadata": {},
   "source": [
    "From this graph it's clear that the choice of $m$ that results in the smallest mean-squared-error is $m=2$, which means the model\n",
    "$$\\hat{y} = 2x$$\n",
    "would be a perfect fit for the data.\n",
    "\n",
    "Okay, but where was the Calculus?\n",
    "\n",
    "Since the slope, $m$, that results in the smalles mean-squared-error, gives us the equation of the line that best fits our data, the cost function\n",
    "$$C(m) = \\frac{1}{4}\\sum_{i=1}^4 (y_i-mx_i)^2$$\n",
    "is the $\\textbf{objective function}$ we will minimize.\n",
    "\n",
    "Since we know that the slope of the tangent line is zero at minimums (and maximums), to minimize this function we take the derivative, set it equal to zero, and solve for the input variable $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28df90e-18e4-46c1-8c0c-1d2a0624e4a4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "C(m) &= \\frac{1}{4}\\sum_{i=1}^4 2(y_i-mx_i)(-x_i)\\\\\n",
    "     &= \\frac{1}{4}\\sum_{i=1}^4 -2x_i(y_i-mx_i)\\\\\n",
    "     &= \\frac{1}{4}\\sum_{i=1}^4 -2x_iy_i+2mx_i^2\\\\\n",
    "     & = \\frac{1}{4}(-2x_1y_1+2mx_1^2 + -2x_2y_2+2mx_2^2+-2x_3y_3+2mx_3^2+-2x_4y_4+2mx_4^2)\\\\\n",
    "     & = -.5x_1y_1+.5mx_1^2 + -.5x_2y_2+.5mx_2^2+-.5x_3y_3+.5mx_3^2+-.5x_4y_4+.5mx_4^2)\\\\\n",
    "     & = -.5(1)(2)+.5m(1)^2 + -.5(2)(4)+.5m(2)^2+-.5(3)(6)+.5m(3)^2+-.5(4)(8)+.5m(4)^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's have the computer take it from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c4a7-28e8-445c-b134-6c103702279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "m = sp.symbols('m')\n",
    "\n",
    "expr = (-.5*1*2 + .5*m*(1)**2- .5*2*4 + .5*m*(2)**2- .5*3*6 + .5*m*(3)**2- .5*4*8 + .5*m*(4)**2)\n",
    "\n",
    "print(\"Simplified:\", sp.simplify(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1116f-09e7-43be-ae78-b79ee15caaea",
   "metadata": {},
   "source": [
    "Yay, that was nice of Python to do that for us. Now we know that\n",
    "$$C'(m) = 15m-30$$\n",
    "so we are going to set this equal to zero, and solve for $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482994e9-c42d-44bf-b44f-74198ddc669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "m = sp.symbols('m')\n",
    "equation = sp.Eq(15*m - 30, 0)  # use sp.Eq instead of Equation\n",
    "solution = sp.solve(equation, m)\n",
    "\n",
    "print(\"Solution: m =\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcafdc0-6e05-45b7-9854-42ef82993118",
   "metadata": {},
   "source": [
    "We see that $m =2$ is a critical point of the function $C(m)$. Since we graphed it we already know this is a minimum, not a maximum, but if we were not sture we could use test points on either side of $m=2$ to classify it.\n",
    "\n",
    "In this case, the mean-squared-error formula and our model (line) were simple enough that we could solve the equation\n",
    "$$\\frac{d}{dm}(C(m))=0$$\n",
    "exactly. \n",
    "\n",
    "This is not usually the case So let's use this example to understand a very powerful technique for finding the minimum of a function, $\\textbf{gradient descent}$.\n",
    "\n",
    "To start gradient descent, we need to choose an initial guess, $m_0$. This should feel similar to Euler's method.\n",
    "\n",
    "Given $m_0 = .75$, use the function below to compute the cost\n",
    "$$C(.75) = \\frac{1}{4}\\sum_{i=1}^4 (y_i-.75x_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5266b-fb9c-4b2a-a26c-f781a7db514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to do the dirty work for us\n",
    "def cost_func(m,input_vals,output_vals):\n",
    "    return (1/len(output_vals))*np.sum((output_vals-m*input_vals)**2)\n",
    "# the input and ouput vals depend on the observed data\n",
    "dollars_spent = np.array([1, 2, 3, 4])\n",
    "sales_revenue = np.array([2, 4, 6, 8])\n",
    "\n",
    "# choose an m\n",
    "curr_m = .75\n",
    "# save the cost for a given choice of m in the variable curr_cost\n",
    "curr_cost = cost_func(curr_m,dollars_spent,sales_revenue)\n",
    "# print the result\n",
    "print(curr_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbd905-b991-42d7-8ffb-c51ead283e8a",
   "metadata": {},
   "source": [
    "This tells us that when $m = .75$, the error (or cost) is $11.7185$. Since our goal is to make the error as close to zero as possible, we need to adjust our guess.\n",
    "\n",
    "However, we won’t just guess randomly, we’ll use the derivative to guide our next step. How?\n",
    "$$m_1 = m_0-C'(m_0)*\\eta.$$\n",
    "\n",
    "Here, $\\eta$ (pronounced \"eta\"), is the \"learning rate.\" When using Euler's method we chose a $\\Delta t$ value, the learning rate functions in the same way by scaling the change arrow. Yet another place where we have decided that in this context we call the derivative or slope of tangent line the gradient.\n",
    "\n",
    "Let's choose $\\eta = .1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a8792-f51d-4ef8-abf8-43c86dd64488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to do the dirty work for us\n",
    "def gradient_func(m):\n",
    "    return 15*m-30\n",
    "\n",
    "# choose an m\n",
    "curr_m = .75\n",
    "# save the slope of the tangent line for a given choice of m\n",
    "gradient = gradient_func(curr_m)\n",
    "# print the result\n",
    "print(curr_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f724b-0525-4430-8aa5-f2f4d22d3fe2",
   "metadata": {},
   "source": [
    "So we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_1 & = .75 -(-18.75*.1)\\\\\n",
    "    &= .75 +1.875 \\\\\n",
    "    & = 2.625\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's take a look at the graph to visualize how exactly gradient descent works.\n",
    "\n",
    "What happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e7943-7561-42da-9e45-6bd11b5b3b91",
   "metadata": {},
   "source": [
    "Let $m_0 = .75, \\eta = .05$ and use the code cell below to complete the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b977c-b369-4f13-a1f5-72c4de1023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_func(m):\n",
    "    return 15*m-30\n",
    "#choice of learning rate\n",
    "eta = .05\n",
    "#current m value\n",
    "curr_m = 1.98\n",
    "#gradient at curr_m\n",
    "gradient = gradient_func(curr_m)\n",
    "print(\"The gradient at m = {} is {}.\".format(curr_m,gradient))\n",
    "#next m\n",
    "next_m = curr_m-(gradient*eta)\n",
    "print(\"The next guess for the slope is {}.\".format(next_m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
